,16.07.2025,16.07.2025,16.07.2025,,,
Model,M2.1 bert-base-uncased,M2.2 bertimbau_hate_speech,M2.3  distilbert-portuguese-cased,,,
Description,CE only,CE only,CE only,,,
Epochs,3,3,3,,,
Alpha,0,0,0,,,
Best Val F1,0.858,0.9228,0.9104,,,
Test F1,0.8428,0.9371,0.9085,,,
Precision ,0.8432,0.9373,0.909,,,
Recall,0.8428,0.9371,0.9085,,,
Val Loss,0.3519,0.2268,0.2448,,,
 Attention-Rationale Alignment Score,,,,,,
,Max length = 512,Max length = 512,Max length = 512,,,
,Batch size = 16,Batch size = 16,Batch size = 16,,,
Note: Split is 70/15/15 not 80/10/10,,,,,,
,,,,,,
Per- class performance,,,,,,
525 samples in each class,,,,,,
,M2.1 Precision,M2.1 Recall,M2.1 F1,M2.2 Precision,M2.2 Recall,M2.2 F1
Offensive,0.8543,0.8266,0.8402,0.9456,0.9276,0.9365
Non-Offensive,0.8321,0.859,0.8453,0.9289,0.9466,0.9377
,,,,,,
,M2.3 Precision,M2.3 Recall,M2.3 F1,,,
Offensive,0.923,0.8914,0.9069,,,
Non-Offensive,0.895,0.9257,0.9101,,,
,,,,,,
,,,,,,
Other hyperparameters,,,,,,
Batch size,16,16,16,,,
LR,2.00E-05,2.00E-05,2.00E-05,,,
Warmup steps,500,500,500,,,
Weight decay,0.01,0.01,0.01,,,
G_A_S,1,1,1,,,
Max lenght,512,512,512,,,
Dropout rate,0.1,0.1,0.1,,,
,,,,,,
Other SRA paremters,,,,,,
Attention layer, -1 for last layer only,,,,,
Attention head,"none, average across heads",,,,,
Rationale threshold,0.5,,,,,
